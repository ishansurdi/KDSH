{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5cdd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from core import (\n",
    "    PathwayDocumentStore, HierarchicalNarrativeMemory,\n",
    "    ClaimExtractor, ConstraintBuilder, MultiHopRetriever,\n",
    "    CausalReasoningEngine, TemporalReasoningEngine,\n",
    "    InconsistencyScorer, ConsistencyClassifier,\n",
    "    load_csv_data, save_results, print_section\n",
    ")\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "print_section(\"COMPLETE PIPELINE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34944f3f",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa85927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'data_dir': '../data',\n",
    "    'novels_dir': '../data/novels',\n",
    "    'results_dir': '../results',\n",
    "    'chunk_size': 1000,\n",
    "    'max_hops': 3,\n",
    "    'top_k_evidence': 5,\n",
    "    'inconsistency_threshold': 0.5\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baba94b1",
   "metadata": {},
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f47e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all components\n",
    "document_store = PathwayDocumentStore(embedding_model=None, chunk_size=CONFIG['chunk_size'])\n",
    "memory = HierarchicalNarrativeMemory()\n",
    "claim_extractor = ClaimExtractor()\n",
    "constraint_builder = ConstraintBuilder()\n",
    "retriever = None  # Will initialize after ingestion\n",
    "scorer = InconsistencyScorer()\n",
    "classifier = ConsistencyClassifier(threshold=CONFIG['inconsistency_threshold'])\n",
    "\n",
    "print(\"✓ All components initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559a5e23",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c96c184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train/test data\n",
    "train_path = os.path.join(CONFIG['data_dir'], 'train.csv')\n",
    "test_path = os.path.join(CONFIG['data_dir'], 'test.csv')\n",
    "\n",
    "if os.path.exists(train_path):\n",
    "    train_data = load_csv_data(train_path)\n",
    "    print(f\"✓ Loaded {len(train_data)} training examples\")\n",
    "else:\n",
    "    train_data = []\n",
    "    print(\"⚠ No train.csv found\")\n",
    "\n",
    "if os.path.exists(test_path):\n",
    "    test_data = load_csv_data(test_path)\n",
    "    print(f\"✓ Loaded {len(test_data)} test examples\")\n",
    "else:\n",
    "    test_data = []\n",
    "    print(\"⚠ No test.csv found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f3188",
   "metadata": {},
   "source": [
    "## Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a12f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(story_id, novel_file, backstory, novel_cache={}):\n",
    "    \"\"\"\n",
    "    Process a single example through the pipeline.\n",
    "    \n",
    "    Returns:\n",
    "        dict with prediction, confidence, rationale\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Load/ingest novel (with caching)\n",
    "        novel_path = os.path.join(CONFIG['novels_dir'], novel_file)\n",
    "        \n",
    "        if novel_file not in novel_cache:\n",
    "            with open(novel_path, 'r', encoding='utf-8') as f:\n",
    "                novel_text = f.read()\n",
    "            \n",
    "            # Ingest\n",
    "            chunk_ids = document_store.ingest_novel(\n",
    "                novel_text=novel_text,\n",
    "                novel_id=novel_file,\n",
    "                metadata={'filename': novel_file}\n",
    "            )\n",
    "            novel_cache[novel_file] = True\n",
    "        \n",
    "        # Step 2: Build memory (simplified for speed)\n",
    "        chunks = []\n",
    "        for chunk_id, doc in document_store.documents.items():\n",
    "            if document_store.chunk_to_doc.get(chunk_id) == novel_file:\n",
    "                chunks.append({\n",
    "                    'chunk_id': chunk_id,\n",
    "                    'text': doc.text,\n",
    "                    'metadata': doc.metadata\n",
    "                })\n",
    "        \n",
    "        local_memory = HierarchicalNarrativeMemory()\n",
    "        local_memory.extract_narrative_from_chunks(chunks, novel_file)\n",
    "        \n",
    "        # Step 3: Extract claims\n",
    "        claims = claim_extractor.extract_claims(backstory)\n",
    "        \n",
    "        # Step 4: Build constraints\n",
    "        constraint_graph = constraint_builder.build_graph(claims)\n",
    "        \n",
    "        # Step 5: Retrieve evidence\n",
    "        local_retriever = MultiHopRetriever(document_store, max_hops=CONFIG['max_hops'])\n",
    "        evidence_map = local_retriever.retrieve_for_claims(\n",
    "            claims=claims,\n",
    "            novel_id=novel_file,\n",
    "            top_k_per_claim=CONFIG['top_k_evidence']\n",
    "        )\n",
    "        \n",
    "        # Step 6: Reasoning\n",
    "        causal_engine = CausalReasoningEngine(local_memory, constraint_graph)\n",
    "        temporal_engine = TemporalReasoningEngine(local_memory, constraint_graph)\n",
    "        \n",
    "        temporal_engine.build_timeline(claims, evidence_map)\n",
    "        temporal_conflicts = temporal_engine.check_temporal_consistency(claims, evidence_map)\n",
    "        causal_conflicts = causal_engine.check_causal_consistency(claims, evidence_map)\n",
    "        \n",
    "        # Step 7: Scoring\n",
    "        score_result = scorer.score_backstory(\n",
    "            claims=claims,\n",
    "            evidence_map=evidence_map,\n",
    "            temporal_conflicts=temporal_conflicts,\n",
    "            causal_conflicts=causal_conflicts,\n",
    "            memory=local_memory\n",
    "        )\n",
    "        \n",
    "        # Step 8: Classification\n",
    "        classification = classifier.classify(\n",
    "            inconsistency_score=score_result['overall_inconsistency'],\n",
    "            temporal_conflicts=temporal_conflicts,\n",
    "            causal_conflicts=causal_conflicts,\n",
    "            evidence_map=evidence_map,\n",
    "            claims=claims\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'story_id': story_id,\n",
    "            'prediction': classification['prediction'],\n",
    "            'confidence': classification['confidence'],\n",
    "            'rationale': classification['rationale']\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {story_id}: {str(e)}\")\n",
    "        return {\n",
    "            'story_id': story_id,\n",
    "            'prediction': 0,\n",
    "            'confidence': 0.5,\n",
    "            'rationale': f\"Error: {str(e)}\"\n",
    "        }\n",
    "\n",
    "print(\"✓ Pipeline function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066e9f6f",
   "metadata": {},
   "source": [
    "## Run on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6496c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process test examples\n",
    "results = []\n",
    "novel_cache = {}\n",
    "\n",
    "print(\"\\nProcessing test examples...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for example in tqdm(test_data[:5]):  # Limit for demo\n",
    "    result = process_example(\n",
    "        story_id=example['story_id'],\n",
    "        novel_file=example['novel_file'],\n",
    "        backstory=example['backstory'],\n",
    "        novel_cache=novel_cache\n",
    "    )\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"\\n{result['story_id']}: {result['prediction']} (conf: {result['confidence']:.2f})\")\n",
    "\n",
    "print(f\"\\n✓ Processed {len(results)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb93751",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83da18ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "output_path = os.path.join(CONFIG['results_dir'], 'results.csv')\n",
    "save_results(results, output_path)\n",
    "\n",
    "print(f\"\\n✓ Results saved to {output_path}\")\n",
    "\n",
    "# Display\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nFinal Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeef260",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cdec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"PIPELINE COMPLETE\")\n",
    "\n",
    "print(\"Results Summary:\")\n",
    "print(f\"  Total processed: {len(results)}\")\n",
    "print(f\"  Consistent (1): {sum(1 for r in results if r['prediction'] == 1)}\")\n",
    "print(f\"  Inconsistent (0): {sum(1 for r in results if r['prediction'] == 0)}\")\n",
    "print(f\"  Average confidence: {sum(r['confidence'] for r in results) / len(results):.2%}\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review results.csv\")\n",
    "print(\"2. Analyze rationales for errors\")\n",
    "print(\"3. Calibrate on full training set\")\n",
    "print(\"4. Run on complete test set\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
