{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa53640",
   "metadata": {},
   "source": [
    "# üèÜ WINNING PIPELINE - KDSH COMPETITION\n",
    "\n",
    "Complete pipeline with Pathway RAG and aggressive inconsistency detection.\n",
    "\n",
    "## Features:\n",
    "- ‚úÖ Pathway document store with vector embeddings\n",
    "- ‚úÖ Multi-stage RAG retrieval\n",
    "- ‚úÖ Enhanced claim extraction (age/date focus)\n",
    "- ‚úÖ Comprehensive conflict detection\n",
    "- ‚úÖ Competition-optimized threshold (0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9971b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from core import (\n",
    "    PathwayDocumentStore, HierarchicalNarrativeMemory,\n",
    "    ClaimExtractor, ConstraintBuilder, MultiHopRetriever,\n",
    "    CausalReasoningEngine, TemporalReasoningEngine,\n",
    "    InconsistencyScorer, ConsistencyClassifier,\n",
    "    load_csv_data, save_results, print_section\n",
    ")\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "print(\"üèÜ WINNING PIPELINE - PATHWAY RAG COMPETITION\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚úì All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13dd620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "CONFIG = {\n",
    "    'chunk_size': 1000,\n",
    "    'max_hops': 3,\n",
    "    'top_k_evidence': 5,\n",
    "    'threshold': 0.30,  # CRITICAL\n",
    "}\n",
    "\n",
    "print(\"üéØ COMPETITION CONFIGURATION:\")\n",
    "print(\"=\"*60)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nüìä KEY FEATURES:\")\n",
    "print(\"  ‚úì Pathway vector search\")\n",
    "print(\"  ‚úì Multi-hop RAG retrieval\")\n",
    "print(\"  ‚úì Enhanced age/date detection\")\n",
    "print(\"  ‚úì Boosted conflict severities (0.88-0.95)\")\n",
    "print(\"  ‚úì Lower threshold for higher recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec5ce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Initialize Components\n",
    "print_section(\"INITIALIZING COMPONENTS\")\n",
    "\n",
    "document_store = PathwayDocumentStore(\n",
    "    embedding_model=None,\n",
    "    chunk_size=CONFIG['chunk_size']\n",
    ")\n",
    "print(\"‚úì Pathway document store initialized\")\n",
    "\n",
    "memory = HierarchicalNarrativeMemory()\n",
    "claim_extractor = ClaimExtractor()\n",
    "constraint_builder = ConstraintBuilder()\n",
    "scorer = InconsistencyScorer()\n",
    "classifier = ConsistencyClassifier(threshold=CONFIG['threshold'])\n",
    "\n",
    "print(\"‚úì All reasoning engines initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ab7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load Test Data\n",
    "print_section(\"LOADING TEST DATA\")\n",
    "\n",
    "test_path = '../data/test.csv'\n",
    "test_data = load_csv_data(test_path)\n",
    "\n",
    "print(f\"‚úì Loaded {len(test_data)} test examples\")\n",
    "print(f\"\\nFirst 3 examples:\")\n",
    "print(\"=\"*60)\n",
    "for i, example in enumerate(test_data[:3], 1):\n",
    "    print(f\"\\n{i}. ID: {example.get('id')}\")\n",
    "    print(f\"   Novel: {example.get('book_name')}\")\n",
    "    print(f\"   Backstory: {example.get('content')[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b8f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Ingest Novels (Pathway)\n",
    "print_section(\"PATHWAY INGESTION\")\n",
    "\n",
    "novels_ingested = {}\n",
    "unique_novels = set(ex.get('book_name') for ex in test_data)\n",
    "print(f\"Unique novels to ingest: {len(unique_novels)}\")\n",
    "\n",
    "for novel_file in unique_novels:\n",
    "    print(f\"\\nüìñ Ingesting: {novel_file}\")\n",
    "    \n",
    "    possible_paths = [\n",
    "        f'../data/novels/{novel_file}.txt',\n",
    "        f'../data/novels/{novel_file}',\n",
    "    ]\n",
    "    \n",
    "    novel_path = None\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            novel_path = path\n",
    "            break\n",
    "    \n",
    "    if novel_path:\n",
    "        with open(novel_path, 'r', encoding='utf-8') as f:\n",
    "            novel_text = f.read()\n",
    "        \n",
    "        chunk_ids = document_store.ingest_novel(\n",
    "            novel_text=novel_text,\n",
    "            novel_id=novel_file,\n",
    "            metadata={'filename': novel_file}\n",
    "        )\n",
    "        \n",
    "        novels_ingested[novel_file] = len(chunk_ids)\n",
    "        print(f\"  ‚úì Created {len(chunk_ids)} chunks\")\n",
    "    else:\n",
    "        print(f\"  ‚ö† Novel file not found: {novel_file}\")\n",
    "\n",
    "print(f\"\\n‚úì Total novels ingested: {len(novels_ingested)}\")\n",
    "print(f\"‚úì Total chunks: {len(document_store.documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3099c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Process All Examples\n",
    "print_section(\"PROCESSING ALL TEST EXAMPLES\")\n",
    "\n",
    "results = []\n",
    "novel_cache = {}\n",
    "\n",
    "for example in tqdm(test_data, desc=\"Processing\"):\n",
    "    story_id = example.get('id')\n",
    "    novel_file = example.get('book_name')\n",
    "    backstory = example.get('content')\n",
    "    \n",
    "    try:\n",
    "        # Build memory (cached)\n",
    "        if novel_file not in novel_cache:\n",
    "            chunks = []\n",
    "            for chunk_id, doc in document_store.documents.items():\n",
    "                if document_store.chunk_to_doc.get(chunk_id) == novel_file:\n",
    "                    chunks.append({\n",
    "                        'chunk_id': chunk_id,\n",
    "                        'text': doc.text,\n",
    "                        'metadata': doc.metadata\n",
    "                    })\n",
    "            \n",
    "            local_memory = HierarchicalNarrativeMemory()\n",
    "            local_memory.extract_narrative_from_chunks(chunks, novel_file)\n",
    "            novel_cache[novel_file] = local_memory\n",
    "        else:\n",
    "            local_memory = novel_cache[novel_file]\n",
    "        \n",
    "        # Extract claims (AGGRESSIVE)\n",
    "        claims = claim_extractor.extract_claims_aggressive(backstory)\n",
    "        \n",
    "        # Build constraints\n",
    "        constraint_graph = constraint_builder.build_graph(claims)\n",
    "        \n",
    "        # Retrieve evidence (Pathway RAG)\n",
    "        retriever = MultiHopRetriever(document_store, max_hops=CONFIG['max_hops'])\n",
    "        evidence_map = {}\n",
    "        for claim in claims:\n",
    "            evidence = retriever.retrieve_evidence(\n",
    "                query=claim.text,\n",
    "                novel_id=novel_file,\n",
    "                top_k_per_hop=CONFIG['top_k_evidence'],\n",
    "                rerank=True\n",
    "            )\n",
    "            evidence_map[claim.claim_id] = evidence\n",
    "        \n",
    "        # Reasoning engines\n",
    "        causal_engine = CausalReasoningEngine(local_memory, constraint_graph)\n",
    "        temporal_engine = TemporalReasoningEngine(local_memory, constraint_graph)\n",
    "        \n",
    "        temporal_engine.build_timeline(claims, evidence_map)\n",
    "        temporal_conflicts = temporal_engine.check_temporal_consistency(claims, evidence_map)\n",
    "        causal_conflicts = causal_engine.check_causal_consistency(claims, evidence_map)\n",
    "        \n",
    "        # Scoring\n",
    "        score_result = scorer.score_backstory(\n",
    "            claims=claims,\n",
    "            evidence_map=evidence_map,\n",
    "            temporal_conflicts=temporal_conflicts,\n",
    "            causal_conflicts=causal_conflicts,\n",
    "            memory=local_memory\n",
    "        )\n",
    "        \n",
    "        # Classification\n",
    "        classification = classifier.classify(\n",
    "            inconsistency_score=score_result['overall_inconsistency'],\n",
    "            temporal_conflicts=temporal_conflicts,\n",
    "            causal_conflicts=causal_conflicts,\n",
    "            evidence_map=evidence_map,\n",
    "            claims=claims\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'id': story_id,\n",
    "            'prediction': classification['prediction'],\n",
    "            'confidence': classification['confidence'],\n",
    "            'rationale': classification['rationale']\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error on {story_id}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        results.append({\n",
    "            'id': story_id,\n",
    "            'prediction': 0,\n",
    "            'confidence': 0.5,\n",
    "            'rationale': f\"Error: {str(e)[:100]}\"\n",
    "        })\n",
    "\n",
    "print(f\"\\n‚úì Processed {len(results)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf46b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Save Results\n",
    "print_section(\"SAVING RESULTS\")\n",
    "\n",
    "output_path = '../results/predictions.csv'\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "save_results(results, output_path)\n",
    "print(f\"‚úì Results saved to: {output_path}\")\n",
    "\n",
    "# Display\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nResults Preview:\")\n",
    "print(results_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3436bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Analysis\n",
    "print_section(\"FINAL ANALYSIS\")\n",
    "\n",
    "total = len(results)\n",
    "consistent = sum(1 for r in results if r['prediction'] == 1)\n",
    "inconsistent = total - consistent\n",
    "\n",
    "print(f\"üìä SUMMARY STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total processed: {total}\")\n",
    "print(f\"Consistent (1): {consistent} ({consistent/total*100:.1f}%)\")\n",
    "print(f\"Inconsistent (0): {inconsistent} ({inconsistent/total*100:.1f}%)\")\n",
    "\n",
    "if results:\n",
    "    avg_conf = sum(r['confidence'] for r in results) / len(results)\n",
    "    print(f\"Average confidence: {avg_conf:.2%}\")\n",
    "\n",
    "print(f\"\\nüéØ TARGET CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Competition baseline: ~36% inconsistent\")\n",
    "print(f\"Our detection rate: {inconsistent/total*100:.1f}%\")\n",
    "\n",
    "if abs(inconsistent/total - 0.36) < 0.05:\n",
    "    print(\"‚úÖ Within target range!\")\n",
    "elif inconsistent/total < 0.31:\n",
    "    print(\"‚ö† Detecting too few - tuning needed\")\n",
    "elif inconsistent/total > 0.41:\n",
    "    print(\"‚ö† Detecting too many - tuning needed\")\n",
    "\n",
    "print(f\"\\nüèÜ READY FOR SUBMISSION!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Submit: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
