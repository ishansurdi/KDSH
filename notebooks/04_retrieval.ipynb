{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593533d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from core import MultiHopRetriever, PathwayDocumentStore, print_section\n",
    "import pickle\n",
    "\n",
    "print_section(\"MULTI-HOP EVIDENCE RETRIEVAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbdf7bd",
   "metadata": {},
   "source": [
    "## Load Previous Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be18d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document store\n",
    "document_store = PathwayDocumentStore(embedding_model=None, chunk_size=1000)\n",
    "document_store.load_index('../results/document_index.json')\n",
    "\n",
    "# Load claims\n",
    "with open('../results/claims.pkl', 'rb') as f:\n",
    "    claims = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(claims)} claims\")\n",
    "print(f\"✓ Loaded document store with {len(document_store.documents)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a35d17e",
   "metadata": {},
   "source": [
    "## Initialize Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509116bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize multi-hop retriever\n",
    "retriever = MultiHopRetriever(\n",
    "    document_store=document_store,\n",
    "    max_hops=3\n",
    ")\n",
    "\n",
    "print(\"✓ Retriever initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b34cbf9",
   "metadata": {},
   "source": [
    "## Retrieve Evidence for Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a75b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve evidence for all claims\n",
    "novel_id = \"evermoor_sample\"\n",
    "\n",
    "evidence_map = retriever.retrieve_for_claims(\n",
    "    claims=claims,\n",
    "    novel_id=novel_id,\n",
    "    top_k_per_claim=5\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Retrieved evidence for {len(evidence_map)} claims\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show summary\n",
    "for claim_id, evidence_list in list(evidence_map.items())[:3]:\n",
    "    print(f\"\\n{claim_id}:\")\n",
    "    print(f\"  Evidence count: {len(evidence_list)}\")\n",
    "    if evidence_list:\n",
    "        print(f\"  Top score: {evidence_list[0].score:.3f}\")\n",
    "        print(f\"  Preview: {evidence_list[0].text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29344b4d",
   "metadata": {},
   "source": [
    "## Test Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27922af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hybrid search for a specific claim\n",
    "test_claim = claims[0] if claims else None\n",
    "\n",
    "if test_claim:\n",
    "    print(f\"\\nTesting hybrid search for claim:\")\n",
    "    print(f\"  {test_claim.text}\")\n",
    "    \n",
    "    hybrid_results = retriever.hybrid_search(\n",
    "        query=test_claim.text,\n",
    "        keywords=test_claim.entities[:3],\n",
    "        novel_id=novel_id,\n",
    "        top_k=5\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nHybrid search results: {len(hybrid_results)}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, result in enumerate(hybrid_results[:3], 1):\n",
    "        print(f\"\\n{i}. Score: {result.score:.3f}\")\n",
    "        print(f\"   Text: {result.text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831f6c51",
   "metadata": {},
   "source": [
    "## Analyze Evidence Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b82734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze evidence quality\n",
    "total_evidence = sum(len(ev) for ev in evidence_map.values())\n",
    "avg_score = sum(\n",
    "    ev.score for evidence_list in evidence_map.values() for ev in evidence_list\n",
    ") / max(total_evidence, 1)\n",
    "\n",
    "print(f\"\\nEvidence Quality Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total evidence pieces: {total_evidence}\")\n",
    "print(f\"Average similarity score: {avg_score:.3f}\")\n",
    "print(f\"Claims with no evidence: {sum(1 for ev in evidence_map.values() if len(ev) == 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3097036",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5038d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evidence map\n",
    "with open('../results/evidence_map.pkl', 'wb') as f:\n",
    "    pickle.dump(evidence_map, f)\n",
    "\n",
    "print(\"\\n✓ Module 4 Complete: Evidence retrieval successful!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
